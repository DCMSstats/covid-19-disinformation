import yaml
import argparse
import datetime as dt
import pandas as pd
import pandas_gbq
import numpy as np

def dataframe_seperator(df, chunk_size):
    """
    A function to seperate a dataframe into smaller chunks by row. A dictionary is returned which contains the original dataframe
    splitm into s no=umber of smaller dataframes with each dataframe containing the same number of columns but with the rows 

    Parameters
    ----------
    df: Pandas Dataframe
        A Pandas Dataframe which is to be split into smaller dataframes
    chunk_size: Int
        How many rows each chunk should contain

    Returns
    ---------
    A dictionary pf pandas dataframes
    """
    dict_of_df = {} 

    chunk_num = test.shape[0] / chunk_size

    chunk_size = int(df.shape[0] / chunk_num)

    for start in range(0, df.shape[0], chunk_size):
        dict_of_df["df_{}".format(start)] = df.iloc[start:start + chunk_size]

    return dict_of_df


def write_in_chunks_gbq(df_dict, gbq_table, project_id):
    """
    Writes the dataframe dictionary, generated by the function dataframe_seperator, to google big query. 
    The dictionary is iterated over with each df being appended to the existing data

    Parameters
    -----------
    df_dict: Dictionary
        A dictionary containing a number of pandas dataframes
    gbq_table: String
        The name of the table to write to
    project_id: String
        The project id in which the gbq_table exists

    Returns
    -------
    None
    """
    df_keys = df_dict.keys()

    for df in df_keys:
        df_to_write = df_dict[df]
        pandas_gbq.to_gbq(df_to_write, gbq_table, project_id=project_id, if_exists="append")

def convert_date(x):
   return dt.datetime.fromtimestamp(x)


def hello(name):
    print("Hello " + name)
    return


def date_range(x):
    early = min(x)
    late = max(x)
    return early, late, print(
        f"The latest date is is {late} and the earliest date is {early}"
    )


def load_config(config_file = "config.yaml"):
    """
    Loads the config file for the reddit pipeline

    parameters
    ----------
    config_file: str
        Name of the config file. Defaults config.ymal

    returns
    -------
    Dictionary of config


    """
    config_yml = open(config_file)
    config = yaml.load(config_yml)
    return config
    
def print_output(topic,comments,*args):
    '''
        Prints to console topics being serached for, 
        comments limited to and how comments are sorted.

        Parameters:
        ----------
        topics being search for, comments limited to and *args

        Returns
        -------
        Prints to console topics being serached for, 
        comments limited to and how comments are sorted.
    '''

    print('\n','Searching reddit for','\n',topic)
    print('\n','Limiting comments to','\n', comments)
    if args:
        print('\n','Sorting comments based on: %s' %args)
    else:
        print('\n',"Sorting comments based on: new")

        
def merge_data_unique(dataset1, dataset2):
    """
    Merged two datasets returning only unique values

    Returns
    -------
    None.

    """

    merged = pd.merge(left=dataset1, right=dataset2, how="outer")

    return merged


def data_to_add(newData, dataStore):
    """
    Filters a pandas dataframe to only new data. 
    Checks the newdata against the datastore
    and removes any rows which are already in the 
    datastore returning only new data to be added 
    to the datastore.

    Parameters
    ----------
    newData : TYPE
        DESCRIPTION.
    dataStore : TYPE
        DESCRIPTION.

    Returns
    -------
    None.

    """

    assert 'id' in newData.columns and 'id' in dataStore.columns, 'both datasets need an id column'

    newID = newData.id.to_numpy()

    oldID = dataStore.id.to_numpy()

    id_filter = [ID in oldID for ID in newID]

    id_filter_reverse = np.invert(id_filter)

    return newData[id_filter_reverse]
